{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27f90907-4864-432d-90f7-07d21312d235",
   "metadata": {},
   "source": [
    "# AI Fairness\n",
    "### -Khushi Arvindkumar Jhanwar | khujhanw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01a8fee-d6ce-4175-be7b-c87511b12fa2",
   "metadata": {},
   "source": [
    "## Importing Libraries and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "18c71131-2e2b-4dad-ad5e-4c0951e82009",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from aif360.datasets import GermanDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.algorithms.preprocessing import Reweighing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8ccafe4c-c3b3-4585-bb8a-c199463f981c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_orig = GermanDataset(\n",
    "    protected_attribute_names=['age'],\n",
    "    privileged_classes=[lambda x: x >= 25],\n",
    "    features_to_drop=['personal_status', 'sex']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34af55c8-a8d2-46e9-9a05-ae96d88cce9d",
   "metadata": {},
   "source": [
    "## Bias Detection using \"age\" as Protected Attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ad39022d-0482-4c0c-9389-023bfdc839fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AGE] Difference in mean outcomes before mitigation = -0.096583\n"
     ]
    }
   ],
   "source": [
    "dataset_orig_train, dataset_orig_test = dataset_orig.split([0.7], shuffle=True)\n",
    "privileged_groups = [{'age': 1}]\n",
    "unprivileged_groups = [{'age': 0}]\n",
    "metric_orig_train = BinaryLabelDatasetMetric(\n",
    "    dataset_orig_train,\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups\n",
    ")\n",
    "\n",
    "print(\"[AGE] Difference in mean outcomes before mitigation = %f\" % metric_orig_train.mean_difference())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177e9e83-3ad2-46d9-b091-1daef068a5bc",
   "metadata": {},
   "source": [
    "## Bias Mitigation using Reweighing (age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5f07c1e2-e883-4942-aafc-a09b30470698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AGE] Difference in mean outcomes after mitigation = -0.000000\n"
     ]
    }
   ],
   "source": [
    "RW = Reweighing(\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups\n",
    ")\n",
    "dataset_transf_train = RW.fit_transform(dataset_orig_train)\n",
    "\n",
    "metric_transf_train = BinaryLabelDatasetMetric(\n",
    "    dataset_transf_train,\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups\n",
    ")\n",
    "\n",
    "print(\"[AGE] Difference in mean outcomes after mitigation = %f\" % metric_transf_train.mean_difference())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50e6f56-7bbb-4654-a3e2-6873e290f74f",
   "metadata": {},
   "source": [
    "## Bias Detection and Mitigation using \"sex\" as Protected Attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bfe49e78-99bd-4563-9144-0b2135f64c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SEX] Difference in mean outcomes before mitigation = -0.059010\n"
     ]
    }
   ],
   "source": [
    "dataset_orig_sex = GermanDataset(\n",
    "    protected_attribute_names=['sex'],\n",
    "    privileged_classes=[['male']],\n",
    "    features_to_drop=['personal_status']  # keep 'sex' for protection\n",
    ")\n",
    "\n",
    "dataset_orig_train_sex, dataset_orig_test_sex = dataset_orig_sex.split([0.7], shuffle=True)\n",
    "\n",
    "privileged_groups_sex = [{'sex': 1}]\n",
    "unprivileged_groups_sex = [{'sex': 0}]\n",
    "\n",
    "metric_orig_train_sex = BinaryLabelDatasetMetric(\n",
    "    dataset_orig_train_sex,\n",
    "    unprivileged_groups=unprivileged_groups_sex,\n",
    "    privileged_groups=privileged_groups_sex\n",
    ")\n",
    "\n",
    "print(\"[SEX] Difference in mean outcomes before mitigation = %f\" % metric_orig_train_sex.mean_difference())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fce00512-cbc5-4e64-bd46-b1a6e1321441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SEX] Difference in mean outcomes after mitigation = 0.000000\n"
     ]
    }
   ],
   "source": [
    "RW_sex = Reweighing(\n",
    "    unprivileged_groups=unprivileged_groups_sex,\n",
    "    privileged_groups=privileged_groups_sex\n",
    ")\n",
    "dataset_transf_train_sex = RW_sex.fit_transform(dataset_orig_train_sex)\n",
    "\n",
    "metric_transf_train_sex = BinaryLabelDatasetMetric(\n",
    "    dataset_transf_train_sex,\n",
    "    unprivileged_groups=unprivileged_groups_sex,\n",
    "    privileged_groups=privileged_groups_sex\n",
    ")\n",
    "\n",
    "print(\"[SEX] Difference in mean outcomes after mitigation = %f\" % metric_transf_train_sex.mean_difference())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e853b4-7fc1-4032-bfb7-8e08dffc6709",
   "metadata": {},
   "source": [
    "# Report\n",
    "\n",
    "## 1. Understanding of Going Through the Tutorial\n",
    "\n",
    "Working through the AI Fairness 360 tutorial gave me hands-on experience in detecting and mitigating bias in a real-world dataset. The tutorial was structured clearly, starting from setting up the environment, importing the German Credit dataset, defining protected attributes, calculating bias metrics, and applying pre-processing bias mitigation. Following the code helped me understand not just the theoretical aspects of fairness in AI, but also how important correct implementation is â€” including careful treatment of attributes like `age` and `sex`.\n",
    "\n",
    "The step-by-step exercises made it easier to visualize how bias appears numerically (e.g., through mean difference values) and how mitigation methods like Reweighing impact those numbers. Overall, the tutorial was a very practical way to connect ethical principles to technical machine learning workflows.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Understanding of Bias and Mitigation\n",
    "\n",
    "Bias in machine learning refers to unfair outcomes where certain groups systematically receive better or worse treatment based on sensitive attributes like age, sex, or race. This bias can be introduced during data collection, model training, or evaluation stages.\n",
    "\n",
    "Pre-processing methods, like the Reweighing algorithm used in this assignment, try to address bias before model training by adjusting the importance (weights) of different examples to balance outcomes. The goal is to ensure that favorable outcomes are not skewed unfairly toward privileged groups. Through the experiment, I learned that a smaller mean difference (closer to zero) indicates that bias has been successfully reduced, improving fairness across groups.\n",
    "\n",
    "The AI Fairness 360 toolkit made it easier to both **measure** bias quantitatively and **mitigate** it using standard algorithms, providing transparency and accountability in machine learning workflows.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Challenges Faced\n",
    "\n",
    "One challenge I faced was setting up the AI Fairness 360 environment. Initially, the module was missing, and I had to install it using Conda and pip commands. Another difficulty was that the German Credit dataset was not automatically available, requiring me to manually download the dataset, place it in the correct directory, and troubleshoot file path issues.\n",
    "\n",
    "In the coding part, defining privileged and unprivileged groups properly was critical when switching the protected attribute from `\"age\"` to `\"sex\"`. Mistakes here initially led to incorrect metric calculations. Restarting the Jupyter kernel and carefully updating the group definitions solved the issue.\n",
    "\n",
    "Despite these challenges, working through them gave me a deeper understanding of how practical issues in machine learning fairness are not just conceptual but often involve detailed technical handling too.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
